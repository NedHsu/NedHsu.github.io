---
title: AI 第15天：深度學習的核心概念
date: 2024-11-15 19:00:00 +0800
categories: [Software, AI]
tags: [AI, Python] 
excerpt: "今天的課程將帶領大家進入深度學習的世界，了解深度學習的核心概念與基礎原理。深度學習是人工智慧與機器學習中的重要分支，透過多層神經網路模擬人類大腦的學習方式。"
---

今天的課程將帶領大家進入深度學習的世界，了解深度學習的核心概念與基礎原理。深度學習是人工智慧與機器學習中的重要分支，透過多層神經網路模擬人類大腦的學習方式。

---

## **課程目標**
1. 了解深度學習的基本概念與運作原理。  
2. 掌握神經網路的核心組成部分與工作流程。  
3. 初步認識反向傳播與梯度下降演算法。

---

## **課程內容**

### **1. 深度學習的概念**

#### **1.1 深度學習是什麼？**
深度學習是一種基於人工神經網路的機器學習技術，擅長於：
- 圖像處理（如人臉識別、自駕車影像分析）。  
- 語音處理（如語音識別、語音合成）。  
- 自然語言處理（如機器翻譯、文本生成）。  

#### **1.2 深度學習與傳統機器學習的區別**
| 特性               | 傳統機器學習           | 深度學習             |
|--------------------|------------------------|----------------------|
| 特徵提取           | 手動設計               | 自動從數據中學習     |
| 算法複雜度         | 依賴經驗               | 通過多層神經網路學習 |
| 適合的數據類型     | 結構化數據             | 圖像、音頻、文本等非結構化數據 |

---

### **2. 人工神經網路 (ANN) 的基本結構**

#### **2.1 神經網路的組成**
1. **輸入層（Input Layer）**：接收數據輸入，例如圖像的像素值或文本的特徵向量。  
2. **隱藏層（Hidden Layers）**：執行數據的特徵提取與學習，多層結構是深度學習的核心。  
3. **輸出層（Output Layer）**：產生預測結果，例如分類標籤或數值。  

#### **2.2 神經元的工作原理**
每個神經元模擬大腦中的神經元，計算公式如下：
\[
z = \sum_{i=1}^{n} w_i x_i + b
\]
\[
a = \sigma(z)
\]
- \( w_i \)：權重，表示每個輸入的影響程度。  
- \( b \)：偏置，用於調整模型的輸出。  
- \( \sigma \)：激活函數，決定神經元的輸出，例如 Sigmoid 或 ReLU。  

---

### **3. 激活函數（Activation Function）**

#### **3.1 常見激活函數**
1. **Sigmoid 函數**  
   \[
   \sigma(z) = \frac{1}{1 + e^{-z}}
   \]  
   適用於概率輸出，但容易造成梯度消失。

2. **ReLU（Rectified Linear Unit）函數**  
   \[
   f(z) = \max(0, z)
   \]  
   解決了梯度消失問題，適合大部分深度學習模型。

3. **Softmax 函數**  
   用於多分類問題，將輸出轉換為概率分布。

---

### **4. 前向傳播與反向傳播**

#### **4.1 前向傳播（Forward Propagation）**
- 數據從輸入層經過隱藏層傳遞到輸出層。  
- 每層的神經元執行加權計算並應用激活函數。  
- 最終產生預測值。

#### **4.2 反向傳播（Backward Propagation）**
- 計算預測值與實際值的損失（Loss）。  
- 反向更新每層的權重與偏置，最小化損失函數。  

---

### **5. 損失函數與梯度下降**

#### **5.1 損失函數（Loss Function）**
損失函數用於衡量模型的預測誤差，常見的有：
1. **均方誤差（MSE）**：用於迴歸問題。  
2. **交叉熵損失（Cross-Entropy Loss）**：用於分類問題。

#### **5.2 梯度下降（Gradient Descent）**
梯度下降是深度學習中常用的優化演算法：
1. 計算損失函數相對於權重的梯度。  
2. 沿梯度方向更新權重，使損失最小化。  

更新公式：  
\[
w = w - \eta \cdot \frac{\partial L}{\partial w}
\]
- \( \eta \)：學習率，決定步伐大小。  
- \( \frac{\partial L}{\partial w} \)：損失函數對權重的偏導數。

---

### **6. 實作：使用 TensorFlow 建立簡單的神經網路**

以下範例展示如何使用 TensorFlow 建立簡單的分類模型。

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# 生成假數據
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 建立模型
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

# 編譯模型
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 訓練模型
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)

# 評估模型
loss, accuracy = model.evaluate(X_test, y_test)
print(f"測試集準確率: {accuracy:.2f}")
```

---

## **課後作業**

1. 練習修改神經網路的結構（層數與神經元數量），觀察模型的性能變化。  
2. 查找不同激活函數的用途，嘗試用其他激活函數（如 Sigmoid、Tanh）。  
3. 思考並回答：  
   - 為什麼需要使用激活函數？  
   - 前向傳播與反向傳播的角色分別是什麼？  
