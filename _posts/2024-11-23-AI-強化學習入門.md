---
title: AI 第23天：強化學習入門  
date: 2024-11-23 19:00:00 +0800
categories: [Software, AI]
tags: [AI, Python] 
excerpt: "強化學習（Reinforcement Learning, RL）是機器學習中一個重要分支，透過試錯的方式，讓代理人（Agent）學習如何在環境中採取最佳行動以最大化回報（Reward）。今天，我們將介紹強化學習的核心概念，並透過實作簡單的 Q-Learning 算法，初步理解強化學習的應用。"
---

強化學習（Reinforcement Learning, RL）是機器學習中一個重要分支，透過試錯的方式，讓代理人（Agent）學習如何在環境中採取最佳行動以最大化回報（Reward）。今天，我們將介紹強化學習的核心概念，並透過實作簡單的 Q-Learning 算法，初步理解強化學習的應用。  

---

## **課程目標**  
1. 理解強化學習的基本框架與核心概念。  
2. 學習 Q-Learning 算法的原理及應用。  
3. 實作一個簡單的強化學習模型來解決經典問題。  

---

## **課程內容**

### **1. 強化學習的核心概念**

#### **1.1 核心元素**  
- **環境（Environment）**：代理人所處的世界，定義行動與回報的規則。  
- **代理人（Agent）**：在環境中執行行動的主體。  
- **狀態（State）**：代理人在某一時刻的環境情況。  
- **行動（Action）**：代理人在當前狀態下可執行的選擇。  
- **回報（Reward）**：代理人執行某行動後所獲得的獎勵或懲罰。  
- **策略（Policy）**：代理人選擇行動的規則或方法。  

#### **1.2 強化學習框架**  
- **目標**：最大化代理人在長期內的累積回報。  
- **馬可夫決策過程（Markov Decision Process, MDP）**：用數學描述強化學習問題的一種方法，包括狀態集合、行動集合、轉移機率和回報函數。  

---

### **2. Q-Learning 算法**

#### **2.1 Q-Learning 的核心原理**  
Q-Learning 是一種強化學習算法，用於學習最優策略。  
- Q 表示狀態-行動值（State-Action Value），即某一狀態下執行某一行動後的預期回報。  
- Q 值的更新公式：  
  \[
  Q(s, a) \gets Q(s, a) + \alpha \cdot [r + \gamma \cdot \max Q(s', a') - Q(s, a)]
  \]
  - \( \alpha \)：學習率  
  - \( \gamma \)：折扣因子，考慮未來回報的重要性  

#### **2.2 Q-Learning 實作：FrozenLake 問題**  

FrozenLake 是 OpenAI Gym 中的一個經典環境，代理人需要在冰面上找到最安全的路徑到達目標點。  

---

### **3. 實作代碼**

#### **3.1 安裝與環境設定**

```bash
pip install gym
```

#### **3.2 啟動環境**

```python
import gym

# 建立 FrozenLake 環境
env = gym.make("FrozenLake-v1", is_slippery=True)
print(f"環境的狀態數量: {env.observation_space.n}")
print(f"行動的種類: {env.action_space.n}")
```

---

#### **3.3 Q-Learning 算法實作**

```python
import numpy as np

# 初始化 Q 表
state_size = env.observation_space.n
action_size = env.action_space.n
q_table = np.zeros((state_size, action_size))

# 超參數
learning_rate = 0.8
discount_factor = 0.95
episodes = 2000
max_steps = 100

# 訓練過程
for episode in range(episodes):
    state = env.reset()[0]  # 初始化狀態
    done = False

    for _ in range(max_steps):
        # 探索與利用策略
        if np.random.uniform(0, 1) < 0.1:
            action = env.action_space.sample()  # 隨機探索
        else:
            action = np.argmax(q_table[state, :])  # 利用現有知識

        # 執行行動並觀察結果
        next_state, reward, done, _, _ = env.step(action)

        # 更新 Q 表
        q_table[state, action] = q_table[state, action] + learning_rate * (
            reward + discount_factor * np.max(q_table[next_state, :]) - q_table[state, action]
        )

        state = next_state

        if done:
            break

print("訓練完成！")
```

---

#### **3.4 測試訓練後的代理人**

```python
total_rewards = 0

for episode in range(10):  # 測試 10 次
    state = env.reset()[0]
    done = False
    print(f"Episode {episode + 1}")

    for _ in range(max_steps):
        action = np.argmax(q_table[state, :])  # 使用訓練後的策略
        next_state, reward, done, _, _ = env.step(action)
        env.render()
        state = next_state
        total_rewards += reward

        if done:
            break

print(f"測試總回報: {total_rewards}")
```

---

### **4. 強化學習應用與挑戰**

#### **4.1 應用場景**
- 自主駕駛：學習如何在動態環境中規劃路徑。
- 遊戲 AI：例如 DeepMind 的 AlphaGo 和 OpenAI 的 Dota2 代理人。  
- 機器人控制：自主學習複雜的操作任務。

#### **4.2 挑戰**
- **探索與利用的平衡**：如何在探索新策略與利用現有知識間取得平衡。  
- **高維度狀態空間**：在高維空間中學習會變得困難。  
- **穩定性問題**：代理人的學習過程可能不穩定。

---

## **課後作業**  
1. 嘗試修改超參數（如學習率或折扣因子），觀察其對結果的影響。  
2. 在 OpenAI Gym 中嘗試其他環境，如 CartPole 或 MountainCar。  
3. 研究 DQN（深度 Q-Learning）並實作更複雜的強化學習模型。  

--- 
