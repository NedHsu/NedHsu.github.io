---
title: AI 第12天：決策樹與隨機森林
date: 2024-11-12 19:00:00 +0800
categories: [Software, AI]
tags: [AI, Python] 
excerpt: "決策樹和隨機森林是強大的機器學習模型，廣泛應用於分類和迴歸問題。今天，我們將學習這兩種方法的基本原理與實現，並了解它們的優勢與局限性。"
---

決策樹和隨機森林是強大的機器學習模型，廣泛應用於分類和迴歸問題。今天，我們將學習這兩種方法的基本原理與實現，並了解它們的優勢與局限性。

---

## **課程目標**
1. 掌握決策樹的基本概念與建構過程。  
2. 學會使用隨機森林進行分類與迴歸。  
3. 理解隨機森林的優勢及其與決策樹的區別。

---

## **課程內容**

### **1. 決策樹**

#### **1.1 核心概念**  
決策樹是基於樹狀結構的模型，通過將數據逐步劃分成不同的子集來進行決策。  
每個節點表示一個特徵的條件判斷，葉節點對應最終的分類或迴歸結果。

#### **1.2 關鍵術語**  
- **節點分裂：** 選擇特徵並設置閾值來分裂數據。  
- **資訊增益：** 衡量特徵分裂的有效性。  
- **純度：** 節點中數據的單一程度（例如 Gini 指數、熵）。  

#### **1.3 實作範例：使用決策樹進行分類**

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.metrics import accuracy_score

# 載入 Iris 數據集
iris = load_iris()
X, y = iris.data, iris.target

# 分割數據
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 建立決策樹模型
tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X_train, y_train)

# 預測與評估
y_pred = tree.predict(X_test)
print(f"決策樹模型準確率: {accuracy_score(y_test, y_pred):.2f}")

# 可視化決策樹的結構
print(export_text(tree, feature_names=iris.feature_names))
```

---

### **2. 隨機森林**

#### **2.1 核心概念**  
隨機森林是一種基於多棵決策樹的集成學習方法。它通過**多樣性**（隨機子樣本與隨機特徵）來提高模型的泛化能力。  
每棵樹獨立訓練，最終通過多數投票（分類）或平均（迴歸）進行預測。

#### **2.2 優勢**  
1. **降低過擬合：** 通過多棵樹的集成，減少單棵樹的偏差。  
2. **處理高維數據：** 能處理大量特徵。  
3. **特徵重要性：** 提供對特徵重要性的評估。  

#### **2.3 實作範例：使用隨機森林進行分類**

```python
from sklearn.ensemble import RandomForestClassifier

# 建立隨機森林模型
forest = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)
forest.fit(X_train, y_train)

# 預測與評估
y_pred_forest = forest.predict(X_test)
print(f"隨機森林模型準確率: {accuracy_score(y_test, y_pred_forest):.2f}")

# 查看特徵重要性
importances = forest.feature_importances_
for name, importance in zip(iris.feature_names, importances):
    print(f"{name}: {importance:.2f}")
```

---

### **3. 決策樹與隨機森林的比較**

| 特點               | 決策樹                     | 隨機森林                 |
|--------------------|---------------------------|-------------------------|
| **模型特性**        | 單一模型，易過擬合         | 集成模型，抗過擬合能力強  |
| **計算成本**        | 訓練快                    | 訓練較慢，但可並行運算    |
| **穩定性**          | 對數據噪聲敏感             | 對數據噪聲較為穩定       |
| **解釋性**          | 易於解釋                  | 個別樹難解釋，但整體效果佳 |

---

## **課後作業**

1. **決策樹實作：**  
   使用 Scikit-learn 中的數據集（如 Wine 或 Breast Cancer），訓練決策樹模型並嘗試調整最大深度（`max_depth`）參數，觀察模型的性能變化。

2. **隨機森林實作：**  
   測試不同的樹數量（`n_estimators`）與特徵數量（`max_features`），比較模型的準確率與穩定性。

3. 思考與回答：  
   - 為什麼隨機森林能夠有效降低過擬合？  
   - 在什麼情況下應選擇使用決策樹而非隨機森林？  
