---
title: AI 第16天：神經網路基礎
date: 2024-11-16 19:00:00 +0800
categories: [Software, AI]
tags: [AI, Python] 
math: true
excerpt: "今天的課程將帶你深入了解神經網路的基本結構與工作原理，為後續學習更複雜的深度學習模型打下堅實的基礎。神經網路模仿了生物神經系統的工作方式，通過層與節點構建一個可訓練的模型。"
---

今天的課程將帶你深入了解神經網路的基本結構與工作原理，為後續學習更複雜的深度學習模型打下堅實的基礎。神經網路模仿了生物神經系統的工作方式，通過層與節點構建一個可訓練的模型。

---

## **課程目標**
1. 掌握神經網路的基本結構與組成要素。  
2. 理解神經網路的運算流程，包括前向傳播與反向傳播。  
3. 能夠使用 Python 建立簡單的全連接神經網路。

---

## **課程內容**

### **1. 神經網路的組成部分**

#### **1.1 神經元（Neuron）**
神經元是神經網路的基本單位，主要功能是接收輸入信號，通過加權運算後應用激活函數產生輸出。

數學表達式：  
$$z = \sum_{i=1}^{n} w_i x_i + b$$
$$a = \sigma(z)$$
- \( x_i \)：輸入特徵。  
- \( w_i \)：對應的權重。  
- \( b \)：偏置（Bias）。  
- \( \sigma(z) \)：激活函數。  

#### **1.2 層（Layer）**
1. **輸入層（Input Layer）**：接收原始數據輸入。  
2. **隱藏層（Hidden Layer）**：進行特徵提取與變換。  
3. **輸出層（Output Layer）**：生成最終預測結果。  

#### **1.3 神經網路的結構**
- 單層神經網路：僅包含輸入層與輸出層，用於簡單的線性問題。  
- 多層感知器（MLP）：包含至少一個隱藏層，用於處理非線性問題。  

---

### **2. 前向傳播（Forward Propagation）**

#### **2.1 運作流程**
1. 輸入數據從輸入層流入隱藏層。  
2. 每個隱藏層的神經元計算加權輸入並應用激活函數。  
3. 最終結果流入輸出層，生成預測值。  

#### **2.2 範例**
假設有 3 個輸入特徵，經過單層神經網路後產生預測值：
```python
import numpy as np

# 假設輸入數據
inputs = np.array([1.0, 2.0, 3.0])

# 權重與偏置
weights = np.array([0.2, 0.8, -0.5])
bias = 0.1

# 計算加權輸入
z = np.dot(inputs, weights) + bias

# 使用 ReLU 激活函數
output = max(0, z)
print(f"輸出結果: {output}")
```

---

### **3. 反向傳播（Backward Propagation）**

#### **3.1 概念**
反向傳播是神經網路的核心學習過程，通過計算損失函數的梯度來更新網路的權重與偏置，使預測結果更接近目標值。

#### **3.2 運作流程**
1. 計算預測值與目標值之間的損失（如均方誤差）。  
2. 從輸出層開始，逐層向前計算權重的梯度。  
3. 使用梯度下降演算法更新權重與偏置。  

更新公式：  
$$w = w - \eta \cdot \frac{\partial L}{\partial w}$$
- \( \eta \)：學習率，用於控制更新幅度。  

---

### **4. 激活函數（Activation Function）**

#### **4.1 常見激活函數**
1. **Sigmoid 函數**：  
   用於將輸出壓縮到 [0, 1] 範圍。  
   $$   \sigma(z) = \frac{1}{1 + e^{-z}} $$

2. **ReLU（Rectified Linear Unit）**：  
   當 \( z > 0 \) 時輸出 \( z \)，否則輸出 0。  
   $$   f(z) = \max(0, z) $$

3. **Tanh 函數**：  
   將輸出壓縮到 [-1, 1] 範圍，比 Sigmoid 表現更穩定。  
   $$   \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} $$

---

### **5. 實作：建立簡單的全連接神經網路**

以下範例展示如何使用 TensorFlow 建立一個簡單的神經網路模型。

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np

# 假設一些輸入數據與對應標籤
X = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])
y = np.array([0, 1, 0])

# 建立神經網路模型
model = Sequential([
    Dense(4, activation='relu', input_shape=(X.shape[1],)),  # 隱藏層
    Dense(1, activation='sigmoid')  # 輸出層
])

# 編譯模型
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 訓練模型
model.fit(X, y, epochs=10, verbose=1)

# 評估模型
loss, accuracy = model.evaluate(X, y)
print(f"準確率: {accuracy:.2f}")
```

---

## **課後作業**

1. 嘗試修改隱藏層的神經元數量，觀察其對模型性能的影響。  
2. 試著使用不同的激活函數（如 Tanh 或 Sigmoid），比較其差異。  
3. 思考並回答：  
   - 為什麼需要多層隱藏層？  
   - ReLU 激活函數在深度學習中為何常用？  
